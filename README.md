## üìÑ README: PNCP Data Miner (Pipeline de Contratos P√∫blicos)

Este projeto implementa um *pipeline* de dados (ETL simplificado) em Python para automatizar a coleta, persist√™ncia e classifica√ß√£o inteligente de licita√ß√µes p√∫blicas extra√≠das diretamente da API do Portal Nacional de Contrata√ß√µes P√∫blicas (PNCP).

O principal objetivo √© utilizar Machine Learning (ML) para automatizar a triagem de milhares de registros, permitindo que analistas ou pesquisadores foquem apenas nos contratos classificados como "Relevantes".

### 1. Arquitetura e Etapas Concisas

O *script* segue uma arquitetura baseada em tr√™s etapas prim√°rias, focando na aplica√ß√£o de Machine Learning para otimizar a filtragem de dados p√∫blicos:

| Etapa | A√ß√£o Principal | Tecnologias Chave | Detalhamento T√©cnico |
| :--- | :--- | :--- | :--- |
| **1. Coleta** | Extra√ß√£o e Persist√™ncia de Dados Brutos. | `requests`, `sqlite3` | Gerenciamento da **Pagina√ß√£o da API** e armazenamento JSON persistente e indexado. |
| **2. Treinamento** | Aprendizado de Padr√µes de Relev√¢ncia. | `scikit-learn`, `joblib` | Cria√ß√£o de um modelo para reconhecer padr√µes em textos de objeto. |
| **3. Classifica√ß√£o** | Filtragem Inteligente dos Dados. | `joblib`, `pandas` | Uso do modelo treinado para classificar automaticamente cada licita√ß√£o como "Relevante" (Classe 1) ou "Irrelevante" (Classe 0). |

---

### 2. Detalhamento T√©cnico da Coleta e Persist√™ncia

#### 2.1. Intera√ß√£o com a API: Pagina√ß√£o e Resili√™ncia

O acesso aos dados da API do PNCP √© feito de forma paginada (ex: 50 registros por p√°gina):

* **Pagina√ß√£o Autom√°tica:** O script interroga a API, identifica o n√∫mero total de p√°ginas (`totalPages`) na resposta e utiliza um *loop* controlado para incrementar o par√¢metro `pagina` na URL, garantindo que todo o volume de dados seja extra√≠do.
* **Mecanismo de *Retry*:** Para manter a estabilidade da coleta, a fun√ß√£o `fetch_com_retry` emprega um **backoff exponencial** (`time.sleep()`). Este mecanismo aumenta o tempo de espera entre tentativas ap√≥s falhas de conex√£o ou limites de taxa (*Rate Limiting*), tornando a coleta resiliente e mais eficaz.

#### 2.2. Persist√™ncia de Dados e Indexa√ß√£o JSON (SQLite/JSON1)

* **Armazenamento Bruto:** Os dados s√£o armazenados no banco de dados local **SQLite** (`pncp_data.db`) em uma coluna prim√°ria (`dados_json`) que preserva o **JSON completo e √≠ntegro** recebido da API.
* **Otimiza√ß√£o com JSON1:** Para acelerar consultas, a extens√£o **JSON1** do SQLite √© utilizada para criar colunas virtuais/geradas (*Generated Columns*) a partir de chaves espec√≠ficas do JSON (ex: `objeto_compra`). Essa t√©cnica permite que √≠ndices tradicionais SQL sejam aplicados diretamente nos metadados do JSON, otimizando drasticamente o desempenho de busca sem a necessidade de *parsing* manual do JSON a cada consulta.

---

### 3. Machine Learning (Classifica√ß√£o de Texto)

A aplica√ß√£o de ML transforma o *pipeline* de um simples extrator de dados para uma ferramenta de intelig√™ncia de neg√≥cios:

1.  **Pr√©-Processamento (NLP):** O campo de texto do objeto da licita√ß√£o √© limpo (remo√ß√£o de pontua√ß√£o, *stopwords* e tokeniza√ß√£o).
2.  **Vetoriza√ß√£o (TF-IDF):** O `TfidfVectorizer` (Term Frequency-Inverse Document Frequency) transforma os textos em vetores num√©ricos. O TF-IDF pesa as palavras, destacando termos que s√£o estatisticamente mais relevantes para o contexto da classifica√ß√£o.
3.  **Treinamento (SGDClassifier):** Um `SGDClassifier` (Stochastic Gradient Descent) √© treinado para mapear os vetores TF-IDF para as classes de relev√¢ncia. O modelo √© persistido como `modelo_relevancia.joblib`.
4.  **Benef√≠cio:** O modelo reduz o tempo de an√°lise manual, classificando automaticamente a relev√¢ncia de novos documentos em segundos.

---

### 4. Pontos Fortes e Fracos do Script

#### ‚úÖ Pontos Fortes

* **Otimiza√ß√£o de Query (JSON1 Indexing):** Uso avan√ßado do SQLite para criar √≠ndices em chaves internas do JSON, garantindo consultas de metadados r√°pidas e eficientes.
* **Resili√™ncia na Coleta:** O *backoff exponencial* na requisi√ß√£o assegura que a coleta de grandes volumes de dados seja conclu√≠da de forma confi√°vel.
* **Filtragem Inteligente (ML):** A aplica√ß√£o de Machine Learning permite uma triagem automatizada e focada, entregando um produto final com alto valor agregado para o analista.

#### ‚ö†Ô∏è Pontos Fracos e Melhorias Necess√°rias

* **ERRO CR√çTICO NO TREINAMENTO (IMEDIATO):**
    * O *output* da c√©lula de treinamento (C√©lula 3) gerou um erro (`The number of classes has to be greater than one; got 1 class`).
    * **Causa T√©cnica:** O `dataset.csv` usado est√° **desbalanceado**, contendo apenas exemplos da classe '1' (Relevante). O classificador precisa de pelo menos duas classes (Relevante e Irrelevante) para aprender.
    * **A√ß√£o Necess√°ria:** **√â indispens√°vel** incluir exemplos de licita√ß√µes irrelevantes (Classe 0) no `dataset.csv` para corrigir este erro e viabilizar a classifica√ß√£o precisa.
* **Par√¢metros *Hardcoded*:** O filtro por `CODIGO_MODALIDADE = 6` (Preg√£o Eletr√¥nico) est√° fixo no c√≥digo.
    * *Melhoria:* Converter par√¢metros chave de busca para argumentos de fun√ß√£o ou vari√°veis de configura√ß√£o.
* **Escalabilidade:** O uso de SQLite √© ideal para prototipagem e uso local, mas n√£o √© recomendado para ambientes de produ√ß√£o.
    * *Melhoria:* Sugere-se migrar para PostgreSQL ou MySQL em uma fase de produ√ß√£o.